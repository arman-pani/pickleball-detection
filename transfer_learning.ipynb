{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j2WyM5qQ8XR",
        "outputId": "3de25d22-037a-4175-95d3-0067f65e332a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "HEIGHT = 288\n",
        "WIDTH = 512\n",
        "\n",
        "def get_model(model_name, num_frame, input_type):\n",
        "    \"\"\" Create model by name and configuration parameters. \"\"\"\n",
        "    if model_name == 'TrackNetV2':\n",
        "        from model import TrackNetV2 as TrackNet\n",
        "    if model_name in ['TrackNetV2']:\n",
        "        model = TrackNet(in_dim=num_frame*3, out_dim=num_frame)\n",
        "    return model\n",
        "\n",
        "def get_frame_unit(frame_list, num_frame):\n",
        "    \"\"\" Generate input sequences from video frames. \"\"\"\n",
        "    batch = []\n",
        "    h, w, _ = frame_list[0].shape\n",
        "    h_ratio, w_ratio = h / HEIGHT, w / WIDTH\n",
        "\n",
        "    def get_unit(frames):\n",
        "        \"\"\" Resize and normalize frames. \"\"\"\n",
        "        resized_frames = np.array([]).reshape(0, HEIGHT, WIDTH)\n",
        "        for img in frames:\n",
        "            img = cv2.resize(img, (WIDTH, HEIGHT))\n",
        "            img = np.moveaxis(img, -1, 0)\n",
        "            resized_frames = np.concatenate((resized_frames, img), axis=0)\n",
        "        return resized_frames\n",
        "\n",
        "    for i in range(0, len(frame_list), num_frame):\n",
        "        frames = get_unit(frame_list[i:i+num_frame]) / 255.0\n",
        "        batch.append(frames)\n",
        "\n",
        "    return torch.FloatTensor(np.array(batch))\n",
        "\n",
        "def get_object_center(heatmap):\n",
        "    \"\"\" Extract ball center coordinates from the heatmap. \"\"\"\n",
        "    if np.amax(heatmap) == 0:\n",
        "        return 0, 0  # No detection\n",
        "    else:\n",
        "        contours, _ = cv2.findContours(heatmap.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        rects = [cv2.boundingRect(ctr) for ctr in contours]\n",
        "        max_rect = max(rects, key=lambda r: r[2] * r[3])\n",
        "        return int(max_rect[0] + max_rect[2] / 2), int(max_rect[1] + max_rect[3] / 2)\n"
      ],
      "metadata": {
        "id": "jkMJYCXSzTUc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChannelAttentionModule(nn.Module):\n",
        "    def __init__(self, channel, ratio=16):\n",
        "        super(ChannelAttentionModule, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.shared_MLP = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channel // ratio, channel, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avgout = self.shared_MLP(self.avg_pool(x))\n",
        "        maxout = self.shared_MLP(self.max_pool(x))\n",
        "        return self.sigmoid(avgout + maxout)\n",
        "\n",
        "class SpatialAttentionModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttentionModule, self).__init__()\n",
        "        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
        "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        out = torch.cat([avgout, maxout], dim=1)\n",
        "        out = self.sigmoid(self.conv2d(out))\n",
        "        return out\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttentionModule(channel)\n",
        "        self.spatial_attention = SpatialAttentionModule()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.channel_attention(x) * x\n",
        "        # out = self.spatial_attention(out) * out\n",
        "        return out\n",
        "\n",
        "class Conv2DBlock(nn.Module):\n",
        "    \"\"\" Conv + ReLU + BN\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, kernel_size, padding='same', bias=True, **kwargs):\n",
        "        super(Conv2DBlock, self).__init__(**kwargs)\n",
        "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Double2DConv(nn.Module):\n",
        "    \"\"\" Conv2DBlock x 2\"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(Double2DConv, self).__init__()\n",
        "        self.conv_1 = Conv2DBlock(in_dim, out_dim, (3, 3))\n",
        "        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        return x\n",
        "\n",
        "class Double2DConv2(nn.Module):\n",
        "    \"\"\" Conv2DBlock x 2\"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(Double2DConv2, self).__init__()\n",
        "        self.conv_1 = Conv2DBlock(in_dim, out_dim, (1, 1))\n",
        "        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "\n",
        "        self.conv_3 = Conv2DBlock(in_dim, out_dim, (3, 3))\n",
        "        self.conv_4 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "\n",
        "        self.conv_5 = Conv2DBlock(in_dim, out_dim, (5, 5))\n",
        "        self.conv_6 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "\n",
        "        self.conv_7 = Conv2DBlock(out_dim*3, out_dim, (3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv_1(x)\n",
        "        x1 = self.conv_2(x1)\n",
        "\n",
        "        x2 = self.conv_3(x)\n",
        "        x2 = self.conv_4(x2)\n",
        "\n",
        "        x3 = self.conv_5(x)\n",
        "        x3 = self.conv_6(x3)\n",
        "\n",
        "        x = torch.cat([x1, x2, x3], dim=1)\n",
        "\n",
        "        x = self.conv_7(x)\n",
        "        x = x + x2\n",
        "\n",
        "        return x\n",
        "\n",
        "class Triple2DConv(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(Triple2DConv, self).__init__()\n",
        "        self.conv_1 = Conv2DBlock(in_dim, out_dim, (3, 3))\n",
        "        self.conv_2 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "        self.conv_3 = Conv2DBlock(out_dim, out_dim, (3, 3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.conv_3(x)\n",
        "        return x\n",
        "\n",
        "class TrackNetV2(nn.Module):\n",
        "    \"\"\" Original structure but less two layers\n",
        "        Total params: 10,161,411\n",
        "        Trainable params: 10,153,859\n",
        "        Non-trainable params: 7,552\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=9, out_dim=3):\n",
        "        super(TrackNetV2, self).__init__()\n",
        "        self.down_block_1 = Double2DConv2(in_dim=in_dim, out_dim=64)\n",
        "        self.down_block_2 = Double2DConv2(in_dim=64, out_dim=128)\n",
        "        self.down_block_3 = Double2DConv2(in_dim=128, out_dim=256)\n",
        "        self.bottleneck = Triple2DConv(in_dim=256, out_dim=512)\n",
        "        self.up_block_1 = Double2DConv(in_dim=768, out_dim=256)\n",
        "        self.up_block_2 = Double2DConv(in_dim=384, out_dim=128)\n",
        "        self.up_block_3 = Double2DConv(in_dim=192, out_dim=64)\n",
        "        self.predictor = nn.Conv2d(64, out_dim, (1, 1))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.cbam1 = CBAM(channel=256) #only channel attention\n",
        "        self.cbam2 = CBAM(channel=128)\n",
        "        self.cbam3 = CBAM(channel=64)\n",
        "\n",
        "        self.cbam0_2 = CBAM(channel=256)\n",
        "        self.cbam1_2 = CBAM(channel=128)\n",
        "        self.cbam2_2 = CBAM(channel=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" model input shape: (F*3, 288, 512), output shape: (F, 288, 512) \"\"\"\n",
        "        x1 = self.down_block_1(x)                                   # (64, 288, 512)\n",
        "        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x1)                 # (64, 144, 256)\n",
        "        x2 = self.down_block_2(x)                                   # (128, 144, 256)\n",
        "        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x2)                 # (128, 72, 128)\n",
        "        x3 = self.down_block_3(x)                                   # (256, 72, 128), one less conv layer\n",
        "        x = nn.MaxPool2d((2, 2), stride=(2, 2))(x3)                 # (256, 36, 64)\n",
        "        x = self.bottleneck(x)                                      # (512, 36, 64)\n",
        "        x3 = self.cbam0_2(x3)\n",
        "        x = torch.cat([nn.Upsample(scale_factor=2)(x), x3], dim=1)  # (768, 72, 128) 256+512\n",
        "\n",
        "        x = self.up_block_1(x)                                      # (256, 72, 128), one less conv layer\n",
        "        x = self.cbam1(x)\n",
        "        x2 = self.cbam1_2(x2)\n",
        "        x = torch.cat([nn.Upsample(scale_factor=2)(x), x2], dim=1)  # (384, 144, 256) 256+128\n",
        "\n",
        "        x = self.up_block_2(x)                                      # (128, 144, 256)\n",
        "        x = self.cbam2(x)\n",
        "        x1 = self.cbam2_2(x1)\n",
        "        x = torch.cat([nn.Upsample(scale_factor=2)(x), x1], dim=1)  # (192, 288, 512) 128+64\n",
        "\n",
        "        x = self.up_block_3(x)                                      # (64, 288, 512)\n",
        "        x = self.cbam3(x)\n",
        "        x = self.predictor(x)                                       # (3, 288, 512)\n",
        "        x = self.sigmoid(x)\n",
        "        return  x\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "Tr = TrackNetV2().cuda()\n",
        "summary(Tr, (9, 288, 512))"
      ],
      "metadata": {
        "id": "ipxttjPXQuT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3493b2b7-ee01-4cfc-966d-2690dcff6c9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 288, 512]             640\n",
            "       BatchNorm2d-2         [-1, 64, 288, 512]             128\n",
            "              ReLU-3         [-1, 64, 288, 512]               0\n",
            "       Conv2DBlock-4         [-1, 64, 288, 512]               0\n",
            "            Conv2d-5         [-1, 64, 288, 512]          36,928\n",
            "       BatchNorm2d-6         [-1, 64, 288, 512]             128\n",
            "              ReLU-7         [-1, 64, 288, 512]               0\n",
            "       Conv2DBlock-8         [-1, 64, 288, 512]               0\n",
            "            Conv2d-9         [-1, 64, 288, 512]           5,248\n",
            "      BatchNorm2d-10         [-1, 64, 288, 512]             128\n",
            "             ReLU-11         [-1, 64, 288, 512]               0\n",
            "      Conv2DBlock-12         [-1, 64, 288, 512]               0\n",
            "           Conv2d-13         [-1, 64, 288, 512]          36,928\n",
            "      BatchNorm2d-14         [-1, 64, 288, 512]             128\n",
            "             ReLU-15         [-1, 64, 288, 512]               0\n",
            "      Conv2DBlock-16         [-1, 64, 288, 512]               0\n",
            "           Conv2d-17         [-1, 64, 288, 512]          14,464\n",
            "      BatchNorm2d-18         [-1, 64, 288, 512]             128\n",
            "             ReLU-19         [-1, 64, 288, 512]               0\n",
            "      Conv2DBlock-20         [-1, 64, 288, 512]               0\n",
            "           Conv2d-21         [-1, 64, 288, 512]          36,928\n",
            "      BatchNorm2d-22         [-1, 64, 288, 512]             128\n",
            "             ReLU-23         [-1, 64, 288, 512]               0\n",
            "      Conv2DBlock-24         [-1, 64, 288, 512]               0\n",
            "           Conv2d-25         [-1, 64, 288, 512]         110,656\n",
            "      BatchNorm2d-26         [-1, 64, 288, 512]             128\n",
            "             ReLU-27         [-1, 64, 288, 512]               0\n",
            "      Conv2DBlock-28         [-1, 64, 288, 512]               0\n",
            "    Double2DConv2-29         [-1, 64, 288, 512]               0\n",
            "           Conv2d-30        [-1, 128, 144, 256]           8,320\n",
            "      BatchNorm2d-31        [-1, 128, 144, 256]             256\n",
            "             ReLU-32        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-33        [-1, 128, 144, 256]               0\n",
            "           Conv2d-34        [-1, 128, 144, 256]         147,584\n",
            "      BatchNorm2d-35        [-1, 128, 144, 256]             256\n",
            "             ReLU-36        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-37        [-1, 128, 144, 256]               0\n",
            "           Conv2d-38        [-1, 128, 144, 256]          73,856\n",
            "      BatchNorm2d-39        [-1, 128, 144, 256]             256\n",
            "             ReLU-40        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-41        [-1, 128, 144, 256]               0\n",
            "           Conv2d-42        [-1, 128, 144, 256]         147,584\n",
            "      BatchNorm2d-43        [-1, 128, 144, 256]             256\n",
            "             ReLU-44        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-45        [-1, 128, 144, 256]               0\n",
            "           Conv2d-46        [-1, 128, 144, 256]         204,928\n",
            "      BatchNorm2d-47        [-1, 128, 144, 256]             256\n",
            "             ReLU-48        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-49        [-1, 128, 144, 256]               0\n",
            "           Conv2d-50        [-1, 128, 144, 256]         147,584\n",
            "      BatchNorm2d-51        [-1, 128, 144, 256]             256\n",
            "             ReLU-52        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-53        [-1, 128, 144, 256]               0\n",
            "           Conv2d-54        [-1, 128, 144, 256]         442,496\n",
            "      BatchNorm2d-55        [-1, 128, 144, 256]             256\n",
            "             ReLU-56        [-1, 128, 144, 256]               0\n",
            "      Conv2DBlock-57        [-1, 128, 144, 256]               0\n",
            "    Double2DConv2-58        [-1, 128, 144, 256]               0\n",
            "           Conv2d-59         [-1, 256, 72, 128]          33,024\n",
            "      BatchNorm2d-60         [-1, 256, 72, 128]             512\n",
            "             ReLU-61         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-62         [-1, 256, 72, 128]               0\n",
            "           Conv2d-63         [-1, 256, 72, 128]         590,080\n",
            "      BatchNorm2d-64         [-1, 256, 72, 128]             512\n",
            "             ReLU-65         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-66         [-1, 256, 72, 128]               0\n",
            "           Conv2d-67         [-1, 256, 72, 128]         295,168\n",
            "      BatchNorm2d-68         [-1, 256, 72, 128]             512\n",
            "             ReLU-69         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-70         [-1, 256, 72, 128]               0\n",
            "           Conv2d-71         [-1, 256, 72, 128]         590,080\n",
            "      BatchNorm2d-72         [-1, 256, 72, 128]             512\n",
            "             ReLU-73         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-74         [-1, 256, 72, 128]               0\n",
            "           Conv2d-75         [-1, 256, 72, 128]         819,456\n",
            "      BatchNorm2d-76         [-1, 256, 72, 128]             512\n",
            "             ReLU-77         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-78         [-1, 256, 72, 128]               0\n",
            "           Conv2d-79         [-1, 256, 72, 128]         590,080\n",
            "      BatchNorm2d-80         [-1, 256, 72, 128]             512\n",
            "             ReLU-81         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-82         [-1, 256, 72, 128]               0\n",
            "           Conv2d-83         [-1, 256, 72, 128]       1,769,728\n",
            "      BatchNorm2d-84         [-1, 256, 72, 128]             512\n",
            "             ReLU-85         [-1, 256, 72, 128]               0\n",
            "      Conv2DBlock-86         [-1, 256, 72, 128]               0\n",
            "    Double2DConv2-87         [-1, 256, 72, 128]               0\n",
            "           Conv2d-88          [-1, 512, 36, 64]       1,180,160\n",
            "      BatchNorm2d-89          [-1, 512, 36, 64]           1,024\n",
            "             ReLU-90          [-1, 512, 36, 64]               0\n",
            "      Conv2DBlock-91          [-1, 512, 36, 64]               0\n",
            "           Conv2d-92          [-1, 512, 36, 64]       2,359,808\n",
            "      BatchNorm2d-93          [-1, 512, 36, 64]           1,024\n",
            "             ReLU-94          [-1, 512, 36, 64]               0\n",
            "      Conv2DBlock-95          [-1, 512, 36, 64]               0\n",
            "           Conv2d-96          [-1, 512, 36, 64]       2,359,808\n",
            "      BatchNorm2d-97          [-1, 512, 36, 64]           1,024\n",
            "             ReLU-98          [-1, 512, 36, 64]               0\n",
            "      Conv2DBlock-99          [-1, 512, 36, 64]               0\n",
            "    Triple2DConv-100          [-1, 512, 36, 64]               0\n",
            "AdaptiveAvgPool2d-101            [-1, 256, 1, 1]               0\n",
            "          Conv2d-102             [-1, 16, 1, 1]           4,096\n",
            "            ReLU-103             [-1, 16, 1, 1]               0\n",
            "          Conv2d-104            [-1, 256, 1, 1]           4,096\n",
            "AdaptiveMaxPool2d-105            [-1, 256, 1, 1]               0\n",
            "          Conv2d-106             [-1, 16, 1, 1]           4,096\n",
            "            ReLU-107             [-1, 16, 1, 1]               0\n",
            "          Conv2d-108            [-1, 256, 1, 1]           4,096\n",
            "         Sigmoid-109            [-1, 256, 1, 1]               0\n",
            "ChannelAttentionModule-110            [-1, 256, 1, 1]               0\n",
            "            CBAM-111         [-1, 256, 72, 128]               0\n",
            "          Conv2d-112         [-1, 256, 72, 128]       1,769,728\n",
            "     BatchNorm2d-113         [-1, 256, 72, 128]             512\n",
            "            ReLU-114         [-1, 256, 72, 128]               0\n",
            "     Conv2DBlock-115         [-1, 256, 72, 128]               0\n",
            "          Conv2d-116         [-1, 256, 72, 128]         590,080\n",
            "     BatchNorm2d-117         [-1, 256, 72, 128]             512\n",
            "            ReLU-118         [-1, 256, 72, 128]               0\n",
            "     Conv2DBlock-119         [-1, 256, 72, 128]               0\n",
            "    Double2DConv-120         [-1, 256, 72, 128]               0\n",
            "AdaptiveAvgPool2d-121            [-1, 256, 1, 1]               0\n",
            "          Conv2d-122             [-1, 16, 1, 1]           4,096\n",
            "            ReLU-123             [-1, 16, 1, 1]               0\n",
            "          Conv2d-124            [-1, 256, 1, 1]           4,096\n",
            "AdaptiveMaxPool2d-125            [-1, 256, 1, 1]               0\n",
            "          Conv2d-126             [-1, 16, 1, 1]           4,096\n",
            "            ReLU-127             [-1, 16, 1, 1]               0\n",
            "          Conv2d-128            [-1, 256, 1, 1]           4,096\n",
            "         Sigmoid-129            [-1, 256, 1, 1]               0\n",
            "ChannelAttentionModule-130            [-1, 256, 1, 1]               0\n",
            "            CBAM-131         [-1, 256, 72, 128]               0\n",
            "AdaptiveAvgPool2d-132            [-1, 128, 1, 1]               0\n",
            "          Conv2d-133              [-1, 8, 1, 1]           1,024\n",
            "            ReLU-134              [-1, 8, 1, 1]               0\n",
            "          Conv2d-135            [-1, 128, 1, 1]           1,024\n",
            "AdaptiveMaxPool2d-136            [-1, 128, 1, 1]               0\n",
            "          Conv2d-137              [-1, 8, 1, 1]           1,024\n",
            "            ReLU-138              [-1, 8, 1, 1]               0\n",
            "          Conv2d-139            [-1, 128, 1, 1]           1,024\n",
            "         Sigmoid-140            [-1, 128, 1, 1]               0\n",
            "ChannelAttentionModule-141            [-1, 128, 1, 1]               0\n",
            "            CBAM-142        [-1, 128, 144, 256]               0\n",
            "          Conv2d-143        [-1, 128, 144, 256]         442,496\n",
            "     BatchNorm2d-144        [-1, 128, 144, 256]             256\n",
            "            ReLU-145        [-1, 128, 144, 256]               0\n",
            "     Conv2DBlock-146        [-1, 128, 144, 256]               0\n",
            "          Conv2d-147        [-1, 128, 144, 256]         147,584\n",
            "     BatchNorm2d-148        [-1, 128, 144, 256]             256\n",
            "            ReLU-149        [-1, 128, 144, 256]               0\n",
            "     Conv2DBlock-150        [-1, 128, 144, 256]               0\n",
            "    Double2DConv-151        [-1, 128, 144, 256]               0\n",
            "AdaptiveAvgPool2d-152            [-1, 128, 1, 1]               0\n",
            "          Conv2d-153              [-1, 8, 1, 1]           1,024\n",
            "            ReLU-154              [-1, 8, 1, 1]               0\n",
            "          Conv2d-155            [-1, 128, 1, 1]           1,024\n",
            "AdaptiveMaxPool2d-156            [-1, 128, 1, 1]               0\n",
            "          Conv2d-157              [-1, 8, 1, 1]           1,024\n",
            "            ReLU-158              [-1, 8, 1, 1]               0\n",
            "          Conv2d-159            [-1, 128, 1, 1]           1,024\n",
            "         Sigmoid-160            [-1, 128, 1, 1]               0\n",
            "ChannelAttentionModule-161            [-1, 128, 1, 1]               0\n",
            "            CBAM-162        [-1, 128, 144, 256]               0\n",
            "AdaptiveAvgPool2d-163             [-1, 64, 1, 1]               0\n",
            "          Conv2d-164              [-1, 4, 1, 1]             256\n",
            "            ReLU-165              [-1, 4, 1, 1]               0\n",
            "          Conv2d-166             [-1, 64, 1, 1]             256\n",
            "AdaptiveMaxPool2d-167             [-1, 64, 1, 1]               0\n",
            "          Conv2d-168              [-1, 4, 1, 1]             256\n",
            "            ReLU-169              [-1, 4, 1, 1]               0\n",
            "          Conv2d-170             [-1, 64, 1, 1]             256\n",
            "         Sigmoid-171             [-1, 64, 1, 1]               0\n",
            "ChannelAttentionModule-172             [-1, 64, 1, 1]               0\n",
            "            CBAM-173         [-1, 64, 288, 512]               0\n",
            "          Conv2d-174         [-1, 64, 288, 512]         110,656\n",
            "     BatchNorm2d-175         [-1, 64, 288, 512]             128\n",
            "            ReLU-176         [-1, 64, 288, 512]               0\n",
            "     Conv2DBlock-177         [-1, 64, 288, 512]               0\n",
            "          Conv2d-178         [-1, 64, 288, 512]          36,928\n",
            "     BatchNorm2d-179         [-1, 64, 288, 512]             128\n",
            "            ReLU-180         [-1, 64, 288, 512]               0\n",
            "     Conv2DBlock-181         [-1, 64, 288, 512]               0\n",
            "    Double2DConv-182         [-1, 64, 288, 512]               0\n",
            "AdaptiveAvgPool2d-183             [-1, 64, 1, 1]               0\n",
            "          Conv2d-184              [-1, 4, 1, 1]             256\n",
            "            ReLU-185              [-1, 4, 1, 1]               0\n",
            "          Conv2d-186             [-1, 64, 1, 1]             256\n",
            "AdaptiveMaxPool2d-187             [-1, 64, 1, 1]               0\n",
            "          Conv2d-188              [-1, 4, 1, 1]             256\n",
            "            ReLU-189              [-1, 4, 1, 1]               0\n",
            "          Conv2d-190             [-1, 64, 1, 1]             256\n",
            "         Sigmoid-191             [-1, 64, 1, 1]               0\n",
            "ChannelAttentionModule-192             [-1, 64, 1, 1]               0\n",
            "            CBAM-193         [-1, 64, 288, 512]               0\n",
            "          Conv2d-194          [-1, 3, 288, 512]             195\n",
            "         Sigmoid-195          [-1, 3, 288, 512]               0\n",
            "================================================================\n",
            "Total params: 15,153,347\n",
            "Trainable params: 15,153,347\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 5.06\n",
            "Forward/backward pass size (MB): 5163.79\n",
            "Params size (MB): 57.81\n",
            "Estimated Total Size (MB): 5226.66\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "# from utils import get_model, get_frame_unit, get_object_center\n",
        "\n",
        "# Directly assign values to variables\n",
        "video_file = '/content/drive/MyDrive/test-video.mp4'  # Specify the path to your input video file\n",
        "model_file = '/content/drive/MyDrive/tracknet-v3-pretrained-model.pt'  # Path to your model file\n",
        "num_frame = 3  # Number of frames to process at a time\n",
        "batch_size = 1  # Batch size for processing\n",
        "save_dir = '/content/drive/MyDrive/pred_result'  # Directory to save the output\n",
        "\n",
        "# Extract video name and format\n",
        "video_name = os.path.splitext(os.path.basename(video_file))[0]\n",
        "video_format = os.path.splitext(video_file)[1][1:]\n",
        "out_video_file = f'{save_dir}/{video_name}_pred.{video_format}'\n",
        "out_csv_file = f'{save_dir}/{video_name}_ball.csv'\n",
        "\n",
        "# Load model checkpoint\n",
        "checkpoint = torch.load(model_file)\n",
        "param_dict = checkpoint['param_dict']\n",
        "model_name = param_dict['model_name']\n",
        "num_frame = param_dict['num_frame']\n",
        "input_type = param_dict['input_type']\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "model = TrackNetV2(in_dim=num_frame * 3, out_dim=num_frame).cuda()  # Manually create model\n",
        "checkpoint = torch.load(model_file)  # Load weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])  # Apply weights\n",
        "model.eval()\n",
        "\n",
        "# Video output configuration\n",
        "if video_format == 'avi':\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "elif video_format == 'mp4':\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "else:\n",
        "    raise ValueError('Invalid video format.')\n",
        "\n",
        "# Write CSV file header\n",
        "with open(out_csv_file, 'w') as f:\n",
        "    f.write('Frame,Visibility,X,Y\\n')\n",
        "\n",
        "# Video capture configuration\n",
        "cap = cv2.VideoCapture(video_file)\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "success = True\n",
        "frame_count = 0\n",
        "num_final_frame = 0\n",
        "ratio = h / HEIGHT\n",
        "out = cv2.VideoWriter(out_video_file, fourcc, fps, (w, h))\n",
        "\n",
        "while success:\n",
        "    print(f'Number of sampled frames: {frame_count}')\n",
        "    # Sample frames to form input sequence\n",
        "    frame_queue = []\n",
        "    for _ in range(num_frame * batch_size):\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "        else:\n",
        "            frame_count += 1\n",
        "            frame_queue.append(frame)\n",
        "\n",
        "    if not frame_queue:\n",
        "        break\n",
        "\n",
        "    # If mini-batch is incomplete\n",
        "    if len(frame_queue) % num_frame != 0:\n",
        "        frame_queue = []\n",
        "        # Record the length of remaining frames\n",
        "        num_final_frame = len(frame_queue) + 1\n",
        "        print(num_final_frame)\n",
        "        # Adjust the sample timestamp of cap\n",
        "        frame_count = frame_count - num_frame * batch_size\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
        "        # Re-sample mini-batch\n",
        "        for _ in range(num_frame * batch_size):\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            else:\n",
        "                frame_count += 1\n",
        "                frame_queue.append(frame)\n",
        "        if len(frame_queue) % num_frame != 0:\n",
        "            continue\n",
        "\n",
        "    x = get_frame_unit(frame_queue, num_frame)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x.cuda())\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "    h_pred = (y_pred > 0.5).astype('uint8') * 255\n",
        "    h_pred = h_pred.reshape(-1, HEIGHT, WIDTH)\n",
        "\n",
        "    for i in range(h_pred.shape[0]):\n",
        "        if num_final_frame > 0 and i < (num_frame * batch_size - num_final_frame - 1):\n",
        "            print('Skipping frame already written to output video.')\n",
        "            continue\n",
        "        else:\n",
        "            img = frame_queue[i].copy()\n",
        "            cx_pred, cy_pred = get_object_center(h_pred[i])\n",
        "            cx_pred, cy_pred = int(ratio * cx_pred), int(ratio * cy_pred)\n",
        "            vis = 1 if cx_pred > 0 and cy_pred > 0 else 0\n",
        "            # Write prediction result\n",
        "            with open(out_csv_file, 'a') as f:\n",
        "                f.write(f'{frame_count - (num_frame * batch_size) + i},{vis},{cx_pred},{cy_pred}\\n')\n",
        "            if cx_pred != 0 or cy_pred != 0:\n",
        "                cv2.circle(img, (cx_pred, cy_pred), 5, (0, 0, 255), -1)\n",
        "            out.write(img)\n",
        "\n",
        "out.release()\n",
        "print('Done.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "collapsed": true,
        "id": "NyZertboy0tI",
        "outputId": "db1e3395-b955-4dd9-a604-c0f38e93ea93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d8cd7205ada4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mnum_final_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mHEIGHT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_video_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfourcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance** **Metrics**"
      ],
      "metadata": {
        "id": "P7SGxMMY1kUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# File paths\n",
        "pred_csv = \"/content/drive/MyDrive/pred_result/test-video_ball.csv\"  # Predicted CSV\n",
        "target_csv = \"/content/drive/MyDrive/target.csv\"  # Manually labeled CSV\n",
        "\n",
        "# Load CSV files\n",
        "pred_df = pd.read_csv(pred_csv)\n",
        "target_df = pd.read_csv(target_csv)\n",
        "\n",
        "# Ensure both dataframes are sorted by frame number\n",
        "pred_df = pred_df.sort_values(by=\"Frame\").reset_index(drop=True)\n",
        "target_df = target_df.sort_values(by=\"Frame\").reset_index(drop=True)\n",
        "\n",
        "# Merge predictions with ground truth based on frame numbers\n",
        "df = pd.merge(target_df, pred_df, on=\"Frame\", suffixes=(\"_true\", \"_pred\"))\n",
        "\n",
        "# Compute errors in X and Y coordinates (only for visible frames)\n",
        "valid_rows = df[\"Visibility_true\"] == 1  # Consider only frames where the ball is visible\n",
        "\n",
        "mse_x = mean_squared_error(df.loc[valid_rows, \"X_true\"], df.loc[valid_rows, \"X_pred\"])\n",
        "mse_y = mean_squared_error(df.loc[valid_rows, \"Y_true\"], df.loc[valid_rows, \"Y_pred\"])\n",
        "mae_x = mean_absolute_error(df.loc[valid_rows, \"X_true\"], df.loc[valid_rows, \"X_pred\"])\n",
        "mae_y = mean_absolute_error(df.loc[valid_rows, \"Y_true\"], df.loc[valid_rows, \"Y_pred\"])\n",
        "\n",
        "# Compute visibility classification metrics\n",
        "accuracy = accuracy_score(df[\"Visibility_true\"], df[\"Visibility_pred\"])\n",
        "precision = precision_score(df[\"Visibility_true\"], df[\"Visibility_pred\"])\n",
        "recall = recall_score(df[\"Visibility_true\"], df[\"Visibility_pred\"])\n",
        "f1 = f1_score(df[\"Visibility_true\"], df[\"Visibility_pred\"])\n",
        "\n",
        "# Print results\n",
        "print(f\"Visibility Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "print(f\"Mean Squared Error (X): {mse_x:.3f}, Mean Squared Error (Y): {mse_y:.3f}\")\n",
        "print(f\"Mean Absolute Error (X): {mae_x:.3f}, Mean Absolute Error (Y): {mae_y:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDiFQnOQ1dle",
        "outputId": "2813f865-72b4-4639-c2ea-33281611f423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visibility Accuracy: 0.839\n",
            "Precision: 0.877, Recall: 0.940, F1-score: 0.908\n",
            "Mean Squared Error (X): 1000252.585, Mean Squared Error (Y): 101904.648\n",
            "Mean Absolute Error (X): 946.832, Mean Absolute Error (Y): 269.149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def generate_gaussian_heatmap(height, width, center, sigma=5):\n",
        "    \"\"\"\n",
        "    Generate a 2D Gaussian heatmap.\n",
        "\n",
        "    Args:\n",
        "        height (int): Height of the heatmap.\n",
        "        width (int): Width of the heatmap.\n",
        "        center (tuple): (x, y) coordinates of the Gaussian center.\n",
        "        sigma (float): Standard deviation of the Gaussian.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Heatmap of shape (1, height, width) with values in [0, 1].\n",
        "    \"\"\"\n",
        "    x_coord = np.arange(width)\n",
        "    y_coord = np.arange(height)\n",
        "    x_grid, y_grid = np.meshgrid(x_coord, y_coord)\n",
        "\n",
        "    # Unpack center coordinates\n",
        "    cx, cy = center\n",
        "    # Calculate the Gaussian\n",
        "    gaussian = np.exp(-((x_grid - cx) ** 2 + (y_grid - cy) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    gaussian = (gaussian - gaussian.min()) / (gaussian.max() - gaussian.min() + 1e-8)\n",
        "\n",
        "    # Return as tensor with shape (1, height, width)\n",
        "    return torch.tensor(gaussian, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "class VideoFrameDataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_file, split=\"train\", transform=None, test_size=0.2, random_state=42, sigma=5):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.sigma = sigma\n",
        "\n",
        "        # Load annotations from CSV\n",
        "        annotations = pd.read_csv(annotation_file)\n",
        "        # Sort by Frame number to ensure sequential order\n",
        "        annotations = annotations.sort_values(\"Frame\").reset_index(drop=True)\n",
        "\n",
        "        # Split into train and test\n",
        "        train_data, test_data = train_test_split(annotations, test_size=test_size, random_state=random_state)\n",
        "        self.annotations = train_data if split == \"train\" else test_data\n",
        "\n",
        "    def __len__(self):\n",
        "        # We need three consecutive frames, so subtract 2\n",
        "        return len(self.annotations) - 2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frames = []\n",
        "        # For each sample, stack 3 consecutive frames\n",
        "        for i in range(3):\n",
        "            row = self.annotations.iloc[idx + i]\n",
        "            # Adjust filename format if needed (here assumed as \"{Frame}.png\")\n",
        "            img_path = os.path.join(self.image_dir, f\"{int(row['Frame'])}.png\")\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)  # Expected shape: (3, H, W)\n",
        "            frames.append(image)\n",
        "\n",
        "        # Stack 3 frames along the channel dimension → (9, H, W)\n",
        "        stacked_frames = torch.cat(frames, dim=0)\n",
        "\n",
        "        # Use the keypoint from the last frame to generate the heatmap target.\n",
        "        last_row = self.annotations.iloc[idx + 2]\n",
        "        # The keypoint coordinates (X, Y) should be scaled according to the transformed image size.\n",
        "        # Here we assume the coordinates in the CSV are given for the resized image.\n",
        "        center = (last_row['X'], last_row['Y'])\n",
        "        # Get height and width from one transformed image\n",
        "        _, H, W = frames[0].shape\n",
        "        heatmap = generate_gaussian_heatmap(H, W, center, sigma=self.sigma)\n",
        "\n",
        "        return stacked_frames, heatmap\n",
        "\n"
      ],
      "metadata": {
        "id": "CrZJByPp6pAZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations (ensure images are resized to 512x288 as per paper)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((288, 512)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Initialize dataset – note that the dataset class should convert coordinate annotations to heatmaps\n",
        "# You may create a helper function to generate a Gaussian heatmap given (X, Y)\n",
        "image_dir = \"/content/drive/MyDrive/data/frames\"\n",
        "annotation_file = \"/content/drive/MyDrive/data/target.csv\"\n",
        "train_dataset = VideoFrameDataset(image_dir, annotation_file, split=\"train\", transform=transform)\n",
        "test_dataset  = VideoFrameDataset(image_dir, annotation_file, split=\"test\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the TrackNetV3 model (ensure your model includes the MCC encoder and improved decoder with Channel Attention)\n",
        "model = TrackNetV2()\n",
        "checkpoint_path = \"/content/drive/MyDrive/data/tracknet-v3-pretrained-model.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# Freeze layers if desired, then unfreeze the decoder layers that incorporate Channel Attention\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# For example, unfreeze the decoder layers (adjust based on your model's attribute names)\n",
        "for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in ['decoder', 'cbam', 'predictor']):\n",
        "        param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define Weighted BCE Loss (WBCE) as in the paper or use a custom loss function\n",
        "# Here, we use a placeholder for the custom loss\n",
        "def weighted_bce_loss(pred, target, weight):\n",
        "    bce = - (1 - weight) * target * torch.log(pred + 1e-8) - weight * (1 - target) * torch.log(1 - pred + 1e-8)\n",
        "    return torch.mean(bce)\n",
        "\n",
        "criterion = weighted_bce_loss  # Replace with your implementation; alternatively, you can adapt nn.BCEWithLogitsLoss\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "num_epochs = 30  # As per the paper\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, heatmaps in train_loader:\n",
        "        images, heatmaps = images.to(device), heatmaps.to(device)  # heatmaps are the converted ground-truth\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # outputs shape: (batch, 3, 288, 512) as heatmaps\n",
        "\n",
        "        # Compute loss; set weight factor based on your strategy\n",
        "        loss = criterion(outputs, heatmaps, weight=0.5)  # Example weight; adjust as needed\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/data/transfer_tracknet_pickleball.pt\")\n"
      ],
      "metadata": {
        "id": "NYU_SlBS7HxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c8e706-22c1-4083-fcc5-9ebab8fc3769"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.0858\n",
            "Epoch [2/30], Loss: 0.0815\n",
            "Epoch [3/30], Loss: 0.0780\n",
            "Epoch [4/30], Loss: 0.0753\n",
            "Epoch [5/30], Loss: 0.0730\n",
            "Epoch [6/30], Loss: 0.0712\n",
            "Epoch [7/30], Loss: 0.0698\n",
            "Epoch [8/30], Loss: 0.0687\n",
            "Epoch [9/30], Loss: 0.0680\n",
            "Epoch [10/30], Loss: 0.0672\n",
            "Epoch [11/30], Loss: 0.0665\n",
            "Epoch [12/30], Loss: 0.0660\n",
            "Epoch [13/30], Loss: 0.0654\n",
            "Epoch [14/30], Loss: 0.0650\n",
            "Epoch [15/30], Loss: 0.0645\n",
            "Epoch [16/30], Loss: 0.0641\n",
            "Epoch [17/30], Loss: 0.0636\n",
            "Epoch [18/30], Loss: 0.0632\n",
            "Epoch [19/30], Loss: 0.0627\n",
            "Epoch [20/30], Loss: 0.0622\n",
            "Epoch [21/30], Loss: 0.0617\n",
            "Epoch [22/30], Loss: 0.0612\n",
            "Epoch [23/30], Loss: 0.0608\n",
            "Epoch [24/30], Loss: 0.0604\n",
            "Epoch [25/30], Loss: 0.0600\n",
            "Epoch [26/30], Loss: 0.0595\n",
            "Epoch [27/30], Loss: 0.0592\n",
            "Epoch [28/30], Loss: 0.0589\n",
            "Epoch [29/30], Loss: 0.0585\n",
            "Epoch [30/30], Loss: 0.0581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Helper functions to compute pixel-wise accuracy and IoU.\n",
        "def compute_metrics(outputs, targets, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute pixel-wise accuracy and IoU for a batch.\n",
        "    Assumes outputs and targets are torch tensors of the same shape.\n",
        "    \"\"\"\n",
        "    # Threshold the outputs to obtain binary predictions\n",
        "    preds = (outputs > threshold).float()\n",
        "\n",
        "    # Compute pixel-wise accuracy\n",
        "    correct = (preds == targets).float().sum()\n",
        "    total = torch.numel(preds)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Compute Intersection over Union (IoU)\n",
        "    # Add a small epsilon to avoid division by zero.\n",
        "    eps = 1e-8\n",
        "    intersection = (preds * targets).sum(dim=[1, 2, 3])\n",
        "    union = (preds + targets - preds * targets).sum(dim=[1, 2, 3]) + eps\n",
        "    iou = (intersection / union).mean()  # Average IoU over batch\n",
        "\n",
        "    return accuracy.item(), iou.item()\n",
        "\n",
        "# Evaluation function that computes loss, accuracy, and IoU.\n",
        "def evaluate_model_metrics(model, data_loader, criterion, device, weight=0.5, threshold=0.5):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_acc = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, heatmaps in data_loader:\n",
        "            images, heatmaps = images.to(device), heatmaps.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Compute loss for the batch\n",
        "            loss = criterion(outputs, heatmaps, weight=weight)\n",
        "            batch_size = images.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Compute metrics for the current batch\n",
        "            acc, iou = compute_metrics(outputs, heatmaps, threshold=threshold)\n",
        "            total_acc += acc * batch_size\n",
        "            total_iou += iou * batch_size\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = total_acc / total_samples\n",
        "    avg_iou = total_iou / total_samples\n",
        "\n",
        "    return avg_loss, avg_acc, avg_iou\n",
        "\n",
        "# Load the original model checkpoint (old model)\n",
        "old_model = TrackNetV2()  # Make sure this matches your model class definition\n",
        "old_checkpoint = torch.load(\"/content/drive/MyDrive/data/tracknet-v3-pretrained-model.pt\", map_location=device)\n",
        "old_model.load_state_dict(old_checkpoint[\"model_state_dict\"])\n",
        "old_model.to(device)\n",
        "\n",
        "# Load the fine-tuned (new) model\n",
        "new_model = TrackNetV2()  # Create a fresh instance if necessary\n",
        "new_model.load_state_dict(torch.load(\"/content/drive/MyDrive/data/transfer_tracknet_pickleball.pt\", map_location=device))\n",
        "new_model.to(device)\n",
        "\n",
        "# Evaluate both models on the test dataset\n",
        "old_loss, old_acc, old_iou = evaluate_model_metrics(old_model, test_loader, criterion, device)\n",
        "new_loss, new_acc, new_iou = evaluate_model_metrics(new_model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Old Model - Test Loss: {old_loss:.4f}, Accuracy: {old_acc:.4f}, IoU: {old_iou:.4f}\")\n",
        "print(f\"New Model - Test Loss: {new_loss:.4f}, Accuracy: {new_acc:.4f}, IoU: {new_iou:.4f}\")\n"
      ],
      "metadata": {
        "id": "sxCbkXPmfvpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c498862-f072-4cd5-d765-6e02b139999e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old Model - Test Loss: 0.0024, Accuracy: 0.9716, IoU: 0.0001\n",
            "New Model - Test Loss: 0.0012, Accuracy: 0.9717, IoU: 0.0000\n"
          ]
        }
      ]
    }
  ]
}